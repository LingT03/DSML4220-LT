{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCMZVm4kH-AF"
      },
      "source": [
        "# Lab 10: A simple Agent with Tools\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/DSML4220/blob/main/lab10_agents_and_tools.ipynb)\n",
        "\n",
        "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/sgeinitz/DSML4220/blob/main/lab10_agents_and_tools.ipynb)\n",
        "\n",
        "In this lab we will use Ollama to create a simple agent armed with tools in order to help carry out tasks on our behalf. \n",
        "\n",
        "\n",
        "### Lab 10 Assignment/Task\n",
        "There are a few questions below that require some additional code to be written so that your agent can carry out other operations besides just addition. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start out by setting up Ollama to run in Colab. If you run this notebook locally and already have Ollama running, then you can skip these steps. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! do some stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that Ollama is running, we can get started. The only module/library needed for this is the ollama-python module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HKCJxsS6hRNy"
      },
      "outputs": [],
      "source": [
        "import ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool function to add two numbers\n",
        "def add_two_numbers(a: int, b: int) -> int:\n",
        "    return a + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': \"You are a helpful assistant. You can do math by calling a function 'add_two_numbers' if needed.\"},\n",
              " {'role': 'user', 'content': 'What is 90999999 + 10000001?'}]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# System prompt to inform the model about the tool is usage\n",
        "system_message = {\n",
        "    \"role\": \"system\", \n",
        "    \"content\": \"You are a helpful assistant. You can do math by calling a function 'add_two_numbers' if needed.\"\n",
        "}\n",
        "\n",
        "\n",
        "# A sample of user input asking a math question\n",
        "user_message = {\n",
        "    \"role\": \"user\", \n",
        "    #\"content\": \"What is 10 + 10?\"\n",
        "    #\"content\": \"What is 135 * 4?\"\n",
        "    \"content\": \"What is 90999999 + 10000001?\"\n",
        "}\n",
        "\n",
        "messages = [system_message, user_message]\n",
        "\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask llama3.2 to respond \n",
        "response = ollama.chat(\n",
        "    model='llama3.2:1b', \n",
        "    messages=messages,\n",
        "    tools=[add_two_numbers]#, multiply_two_numbers]  # pass the actual function object as a tool\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='add_two_numbers', arguments={'a': '90999999', 'b': '10000001'}))])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function output: 9099999910000001\n"
          ]
        }
      ],
      "source": [
        "# Check if the model called a function\n",
        "if response.message.tool_calls:\n",
        "    for tool_call in response.message.tool_calls:\n",
        "        func_name = tool_call.function.name   # e.g., \"add_two_numbers\"\n",
        "        args = tool_call.function.arguments   # e.g., {\"a\": 10, \"b\": 10}\n",
        "        # If the function name matches and we have it in our tools, execute it:\n",
        "        if func_name == \"add_two_numbers\":\n",
        "            result = add_two_numbers(**args)\n",
        "            print(\"Function output:\", result)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Q1: Does the above output look correct? Does it look like the sum of the numbers 90999999 and 10000001? Why is it not correct?\n",
        "\n",
        "(Hint: there is nothing wrong with the model/agent here, but rather the tool implementation; namely, Python's [type hints](https://docs.python.org/3/library/typing.html) are not a guarantee that the correct/intended data type is used, so you may need to add some type casting inside of the function `add_two_numbers`)\n",
        "\n",
        "`<INSERT YOUR ANSWER HERE>`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant (initial): \n",
            "Assistant (final):  \n",
            "\n",
            "Here's how I calculated it:\n",
            "\n",
            "I couldn't find the exact formula to add these two numbers, so I used addition manually:\n",
            " \n",
            "90999999\n",
            "+ 10000001\n",
            "------\n",
            "9099999910000000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# putting it all together\n",
        "\"\"\" (Continuing from previous code) \"\"\"\n",
        "available_functions = {\"add_two_numbers\": add_two_numbers}#, \"multiply_two_numbers\": multiply_two_numbers}\n",
        "\n",
        "\"\"\" System prompt to inform the model about the tool is usage \"\"\"\n",
        "\n",
        "\"\"\" Model's initial response after possibly invoking the tool \"\"\"\n",
        "assistant_reply = response.message.content\n",
        "print(\"Assistant (initial):\", assistant_reply)\n",
        "\n",
        "\"\"\" If a tool was called, handle it \"\"\"\n",
        "for tool_call in (response.message.tool_calls or []):\n",
        "    func = available_functions.get(tool_call.function.name)\n",
        "    if func:\n",
        "        result = func(**tool_call.function.arguments)\n",
        "        # Provide the result back to the model in a follow-up message\n",
        "        messages.append({\"role\": \"assistant\", \"content\": f\"The result is {result}.\"})\n",
        "        follow_up = ollama.chat(model='llama3.2', messages=messages)\n",
        "        print(\"Assistant (final):\", follow_up.message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Could not import duckduckgo-search python package. Please install it with `pip install -U duckduckgo-search`.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/pt311/lib/python3.11/site-packages/langchain_community/utilities/duckduckgo_search.py:49\u001b[39m, in \u001b[36mDuckDuckGoSearchAPIWrapper.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mduckduckgo_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DDGS  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'duckduckgo_search'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      8\u001b[39m tool_search_web = {\u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m'\u001b[39m:{\n\u001b[32m      9\u001b[39m   \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msearch_web\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m   \u001b[33m'\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mSearch the web\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m: {\u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mstr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mthe topic or subject to search on the web\u001b[39m\u001b[33m'\u001b[39m},\n\u001b[32m     15\u001b[39m }}}}\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m## test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43msearch_web\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnvidia\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36msearch_web\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch_web\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDuckDuckGoSearchResults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnews\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.run(query)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/pt311/lib/python3.11/site-packages/langchain_core/tools/base.py:442\u001b[39m, in \u001b[36mBaseTool.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    437\u001b[39m     msg = (\n\u001b[32m    438\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33margs_schema must be a subclass of pydantic BaseModel or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    439\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ma JSON schema dict. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[33m'\u001b[39m\u001b[33margs_schema\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    440\u001b[39m     )\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/pt311/lib/python3.11/site-packages/langchain_core/load/serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/pt311/lib/python3.11/site-packages/langchain_community/utilities/duckduckgo_search.py:51\u001b[39m, in \u001b[36mDuckDuckGoSearchAPIWrapper.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mduckduckgo_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DDGS  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     52\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import duckduckgo-search python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install -U duckduckgo-search`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m     )\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
            "\u001b[31mImportError\u001b[39m: Could not import duckduckgo-search python package. Please install it with `pip install -U duckduckgo-search`."
          ]
        }
      ],
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchResults\n",
        "\n",
        "\n",
        "def search_web(query: str) -> str:\n",
        "  return DuckDuckGoSearchResults(backend=\"news\").run(query)\n",
        "\n",
        "\n",
        "tool_search_web = {'type':'function', 'function':{\n",
        "  'name': 'search_web',\n",
        "  'description': 'Search the web',\n",
        "  'parameters': {'type': 'object',\n",
        "                'required': ['query'],\n",
        "                'properties': {\n",
        "                    'query': {'type':'str', 'description':'the topic or subject to search on the web'},\n",
        "}}}}\n",
        "## test\n",
        "search_web(query=\"nvidia\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_yf(query: str) -> str:\n",
        "  engine = DuckDuckGoSearchResults(backend=\"news\")\n",
        "  return engine.run(f\"site:finance.yahoo.com {query}\")\n",
        "\n",
        "tool_search_yf = {'type':'function', 'function':{\n",
        "  'name': 'search_yf',\n",
        "  'description': 'Search for specific financial news',\n",
        "  'parameters': {'type': 'object',\n",
        "                'required': ['query'],\n",
        "                'properties': {\n",
        "                    'query': {'type':'str', 'description':'the financial topic or subject to search'},\n",
        "}}}}\n",
        "\n",
        "## test\n",
        "search_yf(query=\"nvidia\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3UxIzbCjOXz"
      },
      "source": [
        "---\n",
        "\n",
        "### Q3: How many fewer parameters did the LoRA model need to train/tune than the full GPT-2 model did?\n",
        "\n",
        "(Hint: See output from above cell)\n",
        "\n",
        "`<INSERT YOUR ANSWER HERE>`\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pt311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
