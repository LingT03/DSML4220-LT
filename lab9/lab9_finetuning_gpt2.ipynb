{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCMZVm4kH-AF"
      },
      "source": [
        "# Lab 9: Finetuning GPT-2 with LoRA\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/DSML4220/blob/main/lab9_finetuning_gpt2.ipynb)\n",
        "\n",
        "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/sgeinitz/DSML4220/blob/main/lab9_finetuning_gpt2.ipynb)\n",
        "\n",
        "In this lab we will use GPT-2 for the task of text generation. We'll first quickly compare Greedy Search and (Diverse) Beam Search with GPT-2. Then we'll finetune GPT-2 to generate text that is more explicitly infused with knowledge of Hemingway's book, \"_The Sun also Rises_\", and can generate text in the style of the book.\n",
        "\n",
        "\n",
        "### Lab 9 Assignment/Task\n",
        "There are three questions in this lab. As an added bonus, try downloading your own book from Project Gutenberg to finetune GPT-2 to generate text following your chosen book/author (see this script for help to convert it to a .csv file of sentences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HKCJxsS6hRNy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import GPT2Tokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from peft import LoraModel, LoraConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5WMF0IBZhYA_"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HEHusW_JhmgZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[8888,  318]]), 'attention_mask': tensor([[1, 1]])}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSYvuHZU-4Kf"
      },
      "source": [
        "Let's generate some text from the model using regular Greedy Search (here is the [HuggingFace example documenting this](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.compute_transition_scores.example))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5rqVbKv3rvfv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   262 |  the     | -1.414 | 24.33%\n",
            "|  1110 |  day     | -2.609 | 7.36%\n",
            "|   618 |  when    | -2.010 | 13.41%\n",
            "|   356 |  we      | -1.859 | 15.58%\n",
            "|   460 |  can     | -2.508 | 8.14%\n",
            "|   477 |  all     | -2.752 | 6.38%\n",
            "|   307 |  be      | -2.960 | 5.18%\n",
            "|  6613 |  proud   | -2.135 | 11.82%\n",
            "|   286 |  of      | -0.558 | 57.21%\n",
            "|   674 |  our     | -1.472 | 22.96%\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Print the scores for each token generated with Greedy Search\n",
        "#tokenizer.pad_token_id = tokenizer.eso_token_id\n",
        "outputs = model.generate(**inputs, max_new_tokens=10, return_dict_in_generate=True, output_scores=True)\n",
        "transition_scores = model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, normalize_logits=True\n",
        ")\n",
        "# input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n",
        "# encoder-decoder models, like BART or T5.\n",
        "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
        "    # | token | token string | log probability | probability\n",
        "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2mQQW3GCkBbu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[8888,  318,  262, 1110,  618,  356,  460,  477,  307, 6613,  286,  674]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs['sequences']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGURJAW1_NyV"
      },
      "source": [
        "Let's now use Beam Search (again using [this example from HF](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.compute_transition_scores.example))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MuzkpfBkhdCj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "\n",
        "# Approach 2: Beam Search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=10,\n",
        "    num_beams=6,\n",
        "    #num_beam_groups=3,\n",
        "    #diversity_penalty=5.0,\n",
        "    num_return_sequences=6,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "transition_scores = model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RU90HrOijFmu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[8888,  318,  257, 1049,  640,  284,  307,  257,  636,  286,  674, 2055],\n",
              "        [8888,  318,  257, 1049,  640,  284,  307,  257,  636,  286,  262, 2055],\n",
              "        [8888,  318,  257, 1049,  640,  284,  307,  257,  636,  286,  428, 2055],\n",
              "        [8888,  318,  257, 1049,  640,  284,  307,  257,  636,  286,  340,   13],\n",
              "        [8888,  318,  257, 1049,  640,  284,  307,  257,  636,  286,  428, 1049],\n",
              "        [8888,  318,  257, 1049,  640,  284,  307,  257,  636,  286,  257, 2055]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs['sequences']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bOQUIhTxit1I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq 0: Today is a great time to be a part of our community\n",
            "seq 1: Today is a great time to be a part of the community\n",
            "seq 2: Today is a great time to be a part of this community\n",
            "seq 3: Today is a great time to be a part of it.\n",
            "seq 4: Today is a great time to be a part of this great\n",
            "seq 5: Today is a great time to be a part of a community\n"
          ]
        }
      ],
      "source": [
        "for s, seq in enumerate(outputs['sequences']):\n",
        "  print(f\"seq {s}: {tokenizer.decode(seq)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW7N-MviY5gb"
      },
      "source": [
        "---\n",
        "\n",
        "### Q1: Does the Beam Search above use Diverse Beam Search? If not, change it to use Diverse Beam Search and describe how the output differs.  \n",
        "\n",
        "(Hint: Look a few cells down at the next use of Beam Search, there are two parameters you will need to add, `num_beam_groups`, and `diversity_penalty`)\n",
        "\n",
        "```\n",
        "the outputs above does not use diverse beam search. When initializing the outputs we had `num_beam_groups` and `diversity_penalty` commented out, so we did not enable diverse beam search. In the next cells when we do have them to 3 and 5.0 respectively, we enable diverse beam search. \n",
        "\n",
        "our 6 beams will be divided into 3 groups of 2 beams each and the diversity penalty will encourage the model to explore different tokens and paths, enhancing the diversity of the generated text.\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SXgB8_qorxvL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|  3711 | iced     | -0.646 | 52.43%\n",
            "|   340 |  it      | -1.618 | 19.82%\n",
            "|   510 |  up      | -0.897 | 40.80%\n",
            "|    13 | .        | -1.199 | 30.16%\n",
            "|   198 | \n",
            "        | -1.543 | 21.38%\n",
            "|   198 | \n",
            "        | -0.018 | 98.22%\n",
            "|     1 | \"        | -0.677 | 50.79%\n",
            "|    40 | I        | -1.864 | 15.50%\n",
            "|  1101 | 'm       | -2.006 | 13.45%\n",
            "|   407 |  not     | -1.525 | 21.76%\n",
            "|  1016 |  going   | -1.388 | 24.95%\n",
            "|   284 |  to      | -0.038 | 96.27%\n",
            "|  1309 |  let     | -2.831 | 5.90%\n",
            "|   345 |  you     | -0.957 | 38.42%\n",
            "|   651 |  get     | -2.149 | 11.66%\n"
          ]
        }
      ],
      "source": [
        "prompt = [\"Cohn confronted the bullfighter and \"]\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "max_new_toks = 15\n",
        "# Example 1: Print the scores for each token generated with Greedy Search\n",
        "#outputs = model.generate(**inputs, max_new_tokens=max_new_toks, return_dict_in_generate=True, output_scores=True, do_sample=True, temperature=1)\n",
        "outputs = model.generate(**inputs, max_new_tokens=max_new_toks, return_dict_in_generate=True, output_scores=True, do_sample=False)\n",
        "transition_scores = model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, normalize_logits=True\n",
        ")\n",
        "# input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n",
        "# encoder-decoder models, like BART or T5.\n",
        "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
        "    # | token | token string | log probability | probability\n",
        "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pMa12IGRsVUS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "seq 0: Cohn confronted the bullfighter and iced him up.\n",
            "\n",
            "\"I'm not going to let you get\n",
            "seq 1: Cohn confronted the bullfighter and iced it up.\n",
            "\n",
            "\"I'm not going to let you get\n",
            "seq 2: Cohn confronted the bullfighter and urchin, who had been in a state of shock.\n",
            "\n",
            "\"\n",
            "seq 3: Cohn confronted the bullfighter and ichthyologist, who had been working on the case for more than a\n",
            "seq 4: Cohn confronted the bullfighter and urchin, who had been in a state of shock.\n",
            "\n",
            "The\n",
            "seq 5: Cohn confronted the bullfighter and ichthyologist, who had been working on the case for a while.\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Approach 2: Reconstruct the sequence scores from Beam Search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=max_new_toks,\n",
        "    num_beams=6,\n",
        "    num_beam_groups=3,\n",
        "    diversity_penalty=5.0,\n",
        "    num_return_sequences=6,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        "    temperature=1.0,\n",
        "    #do_sample=True\n",
        ")\n",
        "transition_scores = model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
        ")\n",
        "# If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n",
        "# Tip 1: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n",
        "# use case, you might want to recompute it with `normalize_logits=True`.\n",
        "# Tip 2: the output length does NOT include the input length\n",
        "output_length = np.sum(transition_scores.numpy() < 0, axis=1)\n",
        "length_penalty = model.generation_config.length_penalty\n",
        "reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n",
        "\n",
        "print(np.allclose(outputs.sequences_scores, reconstructed_scores))\n",
        "\n",
        "for s, seq in enumerate(outputs['sequences']):\n",
        "  print(f\"seq {s}: {tokenizer.decode(seq)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQFiVKZjYVZd"
      },
      "source": [
        "Let's now load the raw text from Hemingway's book, _\"The Sun also Rises\"_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Kc-mYE4724pw"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "sentence",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "e3f59daa-234c-41b6-8783-b2de0b9e32fc",
              "rows": [
                [
                  "0",
                  "Robert Cohn was once middleweight boxing champion of Princeton."
                ],
                [
                  "1",
                  "Do not think that I am very much impressed by that as a boxing title, but it meant a lot to Cohn."
                ],
                [
                  "2",
                  "He cared nothing for boxing, in fact he disliked it, but he learned it painfully and thoroughly to counteract the feeling of inferiority and shyness he had felt on being treated as a Jew at Princeton."
                ],
                [
                  "3",
                  "There was a certain inner comfort in knowing he could knock down anybody who was snooty to him, although, being very shy and a thoroughly nice boy, he never fought except in the gym."
                ],
                [
                  "4",
                  "He was Spider Kelly’s star pupil."
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Robert Cohn was once middleweight boxing champ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Do not think that I am very much impressed by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>He cared nothing for boxing, in fact he dislik...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>There was a certain inner comfort in knowing h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>He was Spider Kelly’s star pupil.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence\n",
              "0  Robert Cohn was once middleweight boxing champ...\n",
              "1  Do not think that I am very much impressed by ...\n",
              "2  He cared nothing for boxing, in fact he dislik...\n",
              "3  There was a certain inner comfort in knowing h...\n",
              "4                  He was Spider Kelly’s star pupil."
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "heming = pd.read_csv(\"https://raw.githubusercontent.com/sgeinitz/DSML4220/main/data/sunalsorises.csv\")\n",
        "heming.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aLalprGWvDVf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    Robert Cohn was once middleweight boxing champ...\n",
              "1    Do not think that I am very much impressed by ...\n",
              "2    He cared nothing for boxing, in fact he dislik...\n",
              "3    There was a certain inner comfort in knowing h...\n",
              "4                    He was Spider Kelly’s star pupil.\n",
              "Name: sentence, dtype: object"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = heming['sentence']\n",
        "sentences.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "m_Tf92NAvUQn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        sentence: 'Robert Cohn was once middleweight boxing champion of Princeton.' \n",
            " is tokenized as: [19156, 45005, 373, 1752, 3504, 6551, 21576, 8783, 286, 23173, 13]\n"
          ]
        }
      ],
      "source": [
        "print(f\"        sentence: '{sentences[0]}' \\n is tokenized as: {tokenizer.encode(sentences[0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zlyiUDBpvMgl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "224"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_length = max([len(tokenizer.encode(sentence)) for sentence in sentences])\n",
        "max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0M_fiUgnv-4-"
      },
      "outputs": [],
      "source": [
        "class HemingwayDataset(Dataset):\n",
        "    def __init__(self, txt_list, tokenizer, max_length):\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = []\n",
        "        for txt in txt_list:\n",
        "            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n",
        "                                       max_length=max_length, padding=\"max_length\")\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self): # overload the len() Python built-in function\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx): # overload the [] operator\n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2cDS45JawDMI"
      },
      "outputs": [],
      "source": [
        "dataset = HemingwayDataset(sentences, tokenizer, max_length=max_length)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NEe3658IwLtC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([   27,    91,  9688,  1659,  5239,    91,    29,   464,  4831,   547,\n",
              "           845, 28746,   290,   511,  6698,   547,  6824,   290, 17298,    13,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256]),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw_TgDI973Sj"
      },
      "source": [
        "Notice that above we set the `pad_token_id` to be the same as the `eos_token_id` (i.e. end-of-stream token id). So all of those `50256` entries above are being used as end-of-stream, or end-of-sequence tokens (except the first one, which is denoting the end of the sequence).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "W9JQMBHkwVqs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|endoftext|>'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode([50256])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FkgK6X_gwb5M"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "n_epochs = 2\n",
        "training_args = TrainingArguments(output_dir='~/hemingway_generation', num_train_epochs=n_epochs, logging_steps=100, save_steps=500, do_eval=True,\n",
        "                                  eval_steps=20, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, save_safetensors=False,\n",
        "                                  warmup_steps=10, weight_decay=0.05, logging_dir='~/hemingway_generation/logs', report_to='none')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2UZgnRJGKd7"
      },
      "source": [
        "Let's load GPT-2 and then  take a rough glance at the architecture of GPT (w/ ~130M parameters) by printing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lcM5NuAVXMFY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KezOmgkkGUMM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-2 trainable parameters: 124439808\n"
          ]
        }
      ],
      "source": [
        "def count_trainable_parameters(mod):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, mod.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return params\n",
        "\n",
        "gpt2_params = count_trainable_parameters(model)\n",
        "print(f\"GPT-2 trainable parameters: {gpt2_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mZ92jn8qwzbV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3072' max='3072' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3072/3072 43:42, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.974900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.214100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.221300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.206400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.206500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.202900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.205300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.202900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.203000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.197800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.195800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.195100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.201800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.190400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.173600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.170000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.164700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.181100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.166500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.171000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.173900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.162000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.166200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.180500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.170200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.167700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.160500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.187700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.163300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3072, training_loss=0.2115910556167364, metrics={'train_runtime': 2627.3732, 'train_samples_per_second': 4.676, 'train_steps_per_second': 1.169, 'total_flos': 1404477333504000.0, 'train_loss': 0.2115910556167364, 'epoch': 2.0})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(model=model,  args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset,\n",
        "                  data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
        "                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
        "                                              'labels': torch.stack([f[0] for f in data])})\n",
        "# on Colab this will take 6+hrs w/ cpu or <10min w/ T4 GPU per epoch\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "4WAS8IqWa1Iq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq 0: Cohn confronted the bullfighter and ” “I don’t think so.\n",
            "seq 1: Cohn confronted the bullfighter and ” “I don’t think so,” h\n",
            "seq 2: Cohn confronted the bullfighter and  “I’m going to kill him.\n",
            "seq 3: Cohn confronted the bullfighter and  “I’m not going to let him go.\n",
            "seq 4: Cohn confronted the bullfighter and �����ed him.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Force CPU if MPS is causing issues\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate text\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=max_new_toks,\n",
        "    num_beams=6,\n",
        "    num_beam_groups=3,\n",
        "    diversity_penalty=5.0,\n",
        "    num_return_sequences=5,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        "    temperature=2.0,\n",
        ")\n",
        "\n",
        "transition_scores = model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
        ")\n",
        "\n",
        "output_length = np.sum(transition_scores.cpu().numpy() < 0, axis=1)\n",
        "length_penalty = model.generation_config.length_penalty\n",
        "reconstructed_scores = transition_scores.cpu().sum(axis=1) / (output_length**length_penalty)\n",
        "\n",
        "for s, seq in enumerate(outputs['sequences']):\n",
        "  gen_text = tokenizer.decode(seq)\n",
        "  # remove everything from '<|endoftext|>' on at the end of gen_text\n",
        "  gen_text = gen_text[:gen_text.find('<|endoftext|>')]\n",
        "  print(f\"seq {s}: {gen_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBYHnqkhNuNY"
      },
      "source": [
        "Next, let's use LoRA to fine tune the model. We'll load the model again to ensure that the earlier finetuning is not included."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "grzQdZ0FCJFy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# load the model again so that we can use LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "lQVY2kEGw8Ir"
      },
      "outputs": [],
      "source": [
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc_in\", \"fc_out\", \"wte\", \"c_fc\", \"c_proj\"]\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    inference_mode=False,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=0.01,\n",
        "    fan_in_fan_out=True\n",
        ")\n",
        "\n",
        "lora_model = LoraModel(model, lora_config, \"default\")\n",
        "lora_model.model.tie_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yFmBFdUcDmK8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoraModel(\n",
            "  (model): GPT2LMHeadModel(\n",
            "    (transformer): GPT2Model(\n",
            "      (wte): lora.Embedding(\n",
            "        (base_layer): Embedding(50257, 768)\n",
            "        (lora_dropout): ModuleDict(\n",
            "          (default): Dropout(p=0.01, inplace=False)\n",
            "        )\n",
            "        (lora_A): ModuleDict()\n",
            "        (lora_B): ModuleDict()\n",
            "        (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 16x50257])\n",
            "        (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 768x16])\n",
            "        (lora_magnitude_vector): ModuleDict()\n",
            "      )\n",
            "      (wpe): Embedding(1024, 768)\n",
            "      (drop): Dropout(p=0.1, inplace=False)\n",
            "      (h): ModuleList(\n",
            "        (0-11): 12 x GPT2Block(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): GPT2Attention(\n",
            "            (c_attn): Conv1D(nf=2304, nx=768)\n",
            "            (c_proj): lora.Linear(\n",
            "              (base_layer): Conv1D(nf=768, nx=768)\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.01, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): GPT2MLP(\n",
            "            (c_fc): lora.Linear(\n",
            "              (base_layer): Conv1D(nf=3072, nx=768)\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.01, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=3072, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (c_proj): lora.Linear(\n",
            "              (base_layer): Conv1D(nf=768, nx=3072)\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.01, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "              (lora_magnitude_vector): ModuleDict()\n",
            "            )\n",
            "            (act): NewGELUActivation()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(lora_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "X0i5g63JC4m9"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=lora_model,  args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset,\n",
        "                  data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
        "                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
        "                                              'labels': torch.stack([f[0] for f in data])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rqzHwXzgDBsy"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3072' max='3072' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3072/3072 35:33, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.859300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.319700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.271100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.242800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.236700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.232300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.234500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.232700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.231900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.228200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.224800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.223600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.221500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.229300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.217700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.213500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.216100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.209200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.229400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.211800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.216600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.219100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.205600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.208700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.226200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.214900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.211500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.202200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.233900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.203600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3072, training_loss=0.34453964109222096, metrics={'train_runtime': 2134.6694, 'train_samples_per_second': 5.755, 'train_steps_per_second': 1.439, 'total_flos': 1447176244942848.0, 'train_loss': 0.34453964109222096, 'epoch': 2.0})"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd8ug9Vii9dy"
      },
      "source": [
        "---\n",
        "\n",
        "### Q2: How many more `training_samples_per_second` could the LoRA model get through during finetuning than the original GPT-2 model could?\n",
        "\n",
        "```\n",
        "GPT-2 \n",
        "  - training_samples_per_second: 4.676\n",
        "\n",
        "LoRA\n",
        "    - training_samples_per_second: 5.755\n",
        "\n",
        "differences of 1.079 more samples per second. LoRA is faster than the GPT-2 model because of the low-rank adaptation of the model weights and reduction in the number of trainable parameters.\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "byXjmmgsVgEE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq 0: Cohn confronted the bullfighter and  the bullfighter, and the bullfighter, and the bullfighter\n",
            "seq 1: Cohn confronted the bullfighter and iced him up.\n",
            "seq 2: Cohn confronted the bullfighter and  the bullfighter, and the bullfighter, and the bullfighter\n",
            "seq 3: Cohn confronted the bullfighter and iced him.\n",
            "seq 4: Cohn confronted the bullfighter and ichor with the bullfighter, and the bullfighter had to stop an\n"
          ]
        }
      ],
      "source": [
        "# Ensure the model is moved to the correct device\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "lora_model = lora_model.to(device)\n",
        "\n",
        "# Ensure inputs are moved to the same device\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Use Diverse Beam Search\n",
        "outputs = lora_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=max_new_toks,\n",
        "    num_beams=6,\n",
        "    num_beam_groups=3,\n",
        "    diversity_penalty=5.0,\n",
        "    num_return_sequences=5,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        "    temperature=1.5,\n",
        "    # do_sample=True\n",
        ")\n",
        "\n",
        "transition_scores = lora_model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
        ")\n",
        "\n",
        "output_length = np.sum(transition_scores.cpu().numpy() < 0, axis=1)\n",
        "length_penalty = lora_model.generation_config.length_penalty\n",
        "reconstructed_scores = transition_scores.cpu().sum(axis=1) / (output_length**length_penalty)\n",
        "\n",
        "for s, seq in enumerate(outputs['sequences']):\n",
        "    gen_text = tokenizer.decode(seq)\n",
        "    # Remove everything from '<|endoftext|>' to the end from gen_text\n",
        "    gen_text = gen_text[:gen_text.find('<|endoftext|>')]\n",
        "    print(f\"seq {s}: {gen_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "EM3FTn-fCDGG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-2 trainable parameters: 124439808\n",
            "LoRA trainable parameters: 2585872 (2.08% of GPT-2's trainable parameters)\n"
          ]
        }
      ],
      "source": [
        "lora_params = count_trainable_parameters(lora_model)\n",
        "print(f\"GPT-2 trainable parameters: {gpt2_params}\")\n",
        "print(f\"LoRA trainable parameters: {lora_params} ({(100*lora_params/gpt2_params):.2f}% of GPT-2's trainable parameters)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3UxIzbCjOXz"
      },
      "source": [
        "---\n",
        "\n",
        "### Q3: How many fewer parameters did the LoRA model need to train/tune than the full GPT-2 model did?\n",
        "\n",
        "(Hint: See output from above cell)\n",
        "\n",
        "```\n",
        "GPT-2 has 124,439,808 parameters, while LoRA has only 258,5872 parameter. The reason for this decrease in parameters is that LoRA is fine-tuning only a small subset of GPT-2's parameters. \n",
        "\n",
        "LoRA adds two additional low-rank matrices to the model, which are used to adapt the pre-trained weights. The two matrices have a smaller rank than the original, which will reduce the number of parameters.\n",
        "```\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
